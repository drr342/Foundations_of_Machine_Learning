\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\usepackage{main}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{main}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{enumerate}
\usepackage[linguistics]{forest}
\usepackage{adjustbox}
\usepackage{bbm}
\usepackage{fancyhdr}
%\usepackage[margin=0.5in]{geometry}
%\DeclareMathOperator*{\argmax}{argmax}

\usepackage{listings}
\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\ttfamily\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    columns=fullflexible,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=t,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
}
 
\lstset{style=mystyle}

\title{Foundations of Machine Learning - Fall 2018\\
       \Large Homework 1}
%\graphicspath{{images/}}
\setcitestyle{round, sort, numbers}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Daniel Rivera Ruiz\\
  Department of Computer Science\\
  New York University\\
  \href{mailto:drr342@nyu.edu}{\texttt{drr342@nyu.edu}}\\
}

\pagestyle{fancy}
\fancyhf{}
\rhead{\textbf{Daniel Rivera Ruiz}}
\lhead{\textbf{Foundations of Machine Learning Fall 2018\\Homework 1}}
\cfoot{\thepage}

\begin{document}

%\maketitle

% \cite{} - in-line citation author, year in parenthesis.
% \citep{} - all citation info in parenthesis.

%	\begin{figure}[ht]
%		\centering
%		\frame{
%            \includegraphics[width=1.0\linewidth]{tree.png}
%       }
%		\caption{Classification results for the sentence \textit{"There are slow and repetitive parts, but it has just enough spice to keep it                  interesting."} using the Stanford Sentiment Treebank. As can be seen, sentiment scores are available for each phrase.}
%		\label{tree}
%	\end{figure}
%\thispagestyle{empty}
\appendix
\section{Probability Review}
    \begin{enumerate}[1.]
        \item The first thing we notice is that the problem is referring to a fixed hypothesis $h$ and therefore
        \begin{equation} \label{eq1}
            R(h) = E[\hat{R}(h)]
        \end{equation}
        Now we consider the definition of the variance and substitute equation \ref{eq1}:
        \begin{align}
            \text{Var}[\hat{R}(h)] &= E[(\hat{R}(h)-E[\hat{R}(h)])^2] \nonumber \\
                                   &= E[(\hat{R}(h)-R(h))^2] \label{eq2}
        \end{align}
        Using the identity $E[X^2]=\int_0^{\infty}Pr[X^2>t]dt$ in equation \ref{eq2} we get
        \begin{equation} \label{eq3}
            \text{Var}[\hat{R}(h)] = \int_0^{\infty}Pr[(\hat{R}(h)-R(h))^2>t]dt
        \end{equation}
        Which can be rewritten as follows for any value of $u \in (0, \infty)$:
        \begin{equation} \label{eq4}
            \text{Var}[\hat{R}(h)] = \int_0^uPr[(\hat{R}(h)-R(h))^2>t]dt + \int_u^{\infty}Pr[(\hat{R}(h)-R(h))^2>t]dt
        \end{equation}
        The most basic definition of probability tells us that $P(A) \in [0,1]$ for any event $A$, which implies $P(A) \leq 1$ and so we can write
        \begin{align}
            Pr[(\hat{R}(h)-R(h))^2>t]dt &\leq 1 \nonumber \\
            \int_0^uPr[(\hat{R}(h)-R(h))^2>t]dt &\leq \int_0^udt \nonumber \\
                                                &\leq u \label{eq5}
        \end{align}
        Replacing inequality \ref{eq5} in equation \ref{eq4} we get
        \begin{equation} \label{eq6}
            \text{Var}[\hat{R}(h)] \leq u + \int_u^{\infty}Pr[(\hat{R}(h)-R(h))^2>t]dt
        \end{equation}
        Now we refer to the inequality provided in the problem definition and make $\epsilon^2 = t$:
        \begin{align}
            Pr[|\hat{R}(h)-R(h)|>\epsilon] &\leq 2e^{-2m\epsilon^2} \nonumber \\
                                           &\iff \nonumber \\
            Pr[(\hat{R}(h)-R(h))^2>\epsilon^2] &\leq 2e^{-2m\epsilon^2} \nonumber \\
            Pr[(\hat{R}(h)-R(h))^2>t] &\leq 2e^{-2mt} \label{eq7}                              
        \end{align}
        Replacing inequality \ref{eq7} in inequality \ref{eq6} we get
        \begin{align}
            \text{Var}[\hat{R}(h)] &\leq u + \int_u^{\infty}2e^{-2mt}dt \nonumber \\
                                   &\leq u - \frac{1}{m}e^{-2mt}\bigg\rvert_u^{\infty} \nonumber \\
                                   &\leq u + \frac{1}{m}e^{-2mu} \label{eq8}
        \end{align}
        Finally, we find the value of $u$ that minimizes the upper bound for the variance (right-hand side of inequality \ref{eq8}) by setting its derivative equals to zero and solving for $u$:
        \begin{align}
            \frac{d}{du}\left(u + \frac{1}{m}e^{-2mu}\right) &= 0 \nonumber \\
            1 - 2e^{-2mu} &= 0 \nonumber \\
            e^{2mu} &= 2 \nonumber \\
            u &= \frac{\log(2)}{2m} \label{eq9}
        \end{align}
        Replacing equation \ref{eq9} in inequality \ref{eq8} we get
        \begin{align*}
            \text{Var}[\hat{R}(h)] &\leq \frac{\log(2)}{2m} + \frac{1}{m}e^{-2m\left(\frac{\log(2)}{2m}\right)} \\
                                   &\leq \frac{\log(2)}{2m} + \frac{1}{2m} \\
                                   &\leq \frac{\log(2e)}{2m} \textit \quad {(Q.E.D.)}
        \end{align*}
    \end{enumerate}
\clearpage

\section{PAC Learning}
    \begin{enumerate}[1.]
        \item According to the problem statement, a threshold function $f_c$ is defined as follows:
        \[
        f_c(x)=
        \begin{cases} 
            0: & x < c \\
            1: & x \geq c
        \end{cases}
        \]
        If we consider a sample $S = \{x_i, f_c(x_i)\}$ of size $m$ drawn from a distribution $D$, we can find a separator $\gamma \in \mathbb{R}$ that divides this set in such a way that for all sample points labeled $0$ we have $x_i \leq \gamma$ and for all sample points labeled $1$ we have $x_i > \gamma$.
        We define our learning algorithm $L$ with the following hypothesis $h_S$ based on $\gamma$:
        \[
        h_S(x)=
        \begin{cases} 
            0: & x < \gamma \\
            1: & x \geq \gamma
        \end{cases}
        \]
        Now we define $G$ as the set of valid choices for $\gamma$ i.e., the interval between the rightmost sample point labeled $-1$ and the leftmost sample point labeled $1$. Under this definition, $G$ is a random interval that depends on the sample $S$, and if it is narrow enough then $\gamma$ will be very close to the true value of $c$ and our algorithm will have a small error $R$, since our algorithm can only make mistakes within $G$.\\
        Now we notice that $R(h_S) = Pr_{x \sim D}[h_S(x) \neq f_c(x)]$ is equivalent to the amount of weight that the distribution $D$ puts in the interval between $c$ and $\gamma$, and so we would like to find an upper bound on $Pr[R(h_S) > \epsilon]$.\\
        First we set $\epsilon > 0$ and define $c_1$ and $c_2$ as follows:
        \begin{align*}
            c_1 = \max_{v < c}(Pr_{x \sim D}[v \leq x \leq c] &\geq \epsilon)\\
            c_2 = \min_{v > c}(Pr_{x \sim D}[v \leq x \leq c] &\geq \epsilon)
        \end{align*}
        If the input sample $S$ contains at least one point in $C_1=[c_1,c]$ and one point in $C_2=[c,c_2]$, then our algorithm must output a threshold value $\gamma \in [c_1, c_2]$ and it is easy to see that any such value will have error no more than $\epsilon$ with respect to $c$. Therefore, $Pr[R(h_S) > \epsilon]$ implies that the sample $S$ does not contain a point in at least one of these regions and we write:
        \begin{equation*}
            Pr[R(h_S) > \epsilon] \leq Pr[x_1 \notin C_1 \wedge \dots \wedge x_n \notin C_1] + Pr[x_1 \notin C_2 \wedge \dots \wedge x_n \notin C_2]
        \end{equation*}
        Then we observe that
        \begin{align*}
            Pr[x_1 \notin C_1 \wedge \dots \wedge x_n \notin C_1] &= \prod_{i=1}^{m}Pr[x_i \notin C_1] \\
            &\leq (1 - \epsilon)^m \\
            &\leq e^{-\epsilon m}
        \end{align*}
        With a similar procedure we can bound the probability that no point in the sample falls in $C_2$ and therefore we have that
        \begin{equation*}
            Pr[R(h_S) > \epsilon] \leq 2e^{-\epsilon m}
        \end{equation*}
        To finish the proof we set $\delta$ to match the upper bound and solve for $m$:
        \begin{align*}
            2e^{-\epsilon m} &\leq \delta \\
            m &\geq \frac{1}{\epsilon}\ln\left(\frac{2}{\delta}\right)
        \end{align*}
        With this we can assure that $L$ is a PAC-learning algorithm for $C$, and therefore for a sample $S$ of size $m \geq \frac{1}{\epsilon}\ln\left(\frac{2}{\delta}\right)$, $L$ will return a hypothesis $h_S$ such that $Pr[R(h_S) \leq \epsilon] \geq 1 - \delta$.
        \clearpage
        \item Let us consider  a concept function $f_c$ of the following form:
        \[
        f_{c_xc_y}(x, y)=
        \begin{cases} 
            0: & x < c_x,y < c_y \\
            1: & x \geq c_x,y \geq c_y
        \end{cases}
        \]
        If we look only at one of the coordinates for the $m$ sample points in $S$, we can apply our PAC-learning algorithm $L$ from the previous exercise:
        \begin{align*}
            Pr[R(h_{S_x}) &> \epsilon] \leq 2e^{-\epsilon m} \\
            Pr[R(h_{S_y}) &> \epsilon] \leq 2e^{-\epsilon m}
        \end{align*}
        Where $h_{S_x}$ and $h_{S_y}$ are the hypotheses returned by $L$ for the $x$-coordinate and $y$-coordinate, respectively.\\
        Now we can define our hypothesis $h_S$:
        \begin{equation*}
            h_S(x,y) = h_{S_x} \wedge h_{S_y}
        \end{equation*}
        Under this definition, it is easy to see that $h_S$ will only make a mistake when $h_{S_x}$ or $h_{S_y}$ make a mistake. Thinking of it in terms of the probability $Pr[R(h_S) > \epsilon]$ that we are trying to upper bound we write:
        \begin{align*}
            Pr[R(h_S) > \epsilon] &= Pr[R(h_{S_x}) > \epsilon] \lor Pr[R(h_{S_y}) > \epsilon] \\
                                  &= Pr[R(h_{S_x}) > \epsilon] + Pr[R(h_{S_y}) > \epsilon] \\
                                  &\leq 4e^{-\epsilon m}
        \end{align*}
        The first step follows from the fact that $x$ and $y$ are the coordinates of the sample points and therefore random and independent. The second step is a simple substitution that follows from adding the results for $h_{S_x}$ and $h_{S_y}$.\\
        Now we are ready to finish our proof by setting $\delta$ to match the upper bound and solving for $m$:
        \begin{align*}
            4e^{-\epsilon m} &\leq \delta \\
            m &\geq \frac{1}{\epsilon}\ln\left(\frac{4}{\delta}\right)
        \end{align*}
        To generalize our learning algorithm to other concept functions in $C_2$, we simply have to keep track of the minimum coordinates of positive and negative sample points in $S$ and define the partial hypotheses accordingly:
    	\begin{table}[ht]
    		\centering
    		\begin{tabular}{llccc}
    			\toprule
    			Condition $x$ & Condition $y$ & Positive concept $f_{c_xc_y}$ & Positive $h_{S_x}$ & Positive $h_{S_y}$ \\
    			\midrule
    			$\min_x^- < \min_x^+$ & $\min_y^- < \min_y^+$ & $x \geq c_x,y \geq c_y$ & $x \geq \gamma_x$ & $y \geq \gamma_y$ \\
    			$\min_x^- < \min_x^+$ & $\min_y^- > \min_y^+$ & $x \geq c_x,y \leq c_y$ & $x \geq \gamma_x$ & $y \leq \gamma_y$ \\
    			$\min_x^- > \min_x^+$ & $\min_y^- < \min_y^+$ & $x \leq c_x,y \geq c_y$ & $x \leq \gamma_x$ & $y \geq \gamma_y$ \\
    			$\min_x^- > \min_x^+$ & $\min_y^- > \min_y^+$ & $x \leq c_x,y \leq c_y$ & $x \leq \gamma_x$ & $y \leq \gamma_y$ \\
    			\bottomrule
    		\end{tabular}
        \end{table}\\
        Where the notation $\min_x^-$ means the minimum value in the $x$-coordinate for all points classified as negative in the sample $S$. With these definitions for the partial hypotheses $h_{S_x}$ and $h_{S_y}$, we can keep our final hypothesis definition constant for all concepts in $C_2$: $h_S(x,y) = h_{S_x} \wedge h_{S_y}$. 
    \end{enumerate}
\end{document}